name: KuzuMemory Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run regression tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  KUZU_VERSION: '0.0.8'

jobs:
  # Unit Tests - Fast feedback
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ \
          --verbose \
          --cov=kuzu_memory \
          --cov-report=xml \
          --cov-report=term-missing \
          --junit-xml=test-results-unit.xml \
          --tb=short
    
    - name: Upload unit test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: test-results-unit.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.python-version == '3.9'
      with:
        file: ./coverage.xml
        flags: unit
        name: unit-tests

  # Integration Tests - More comprehensive
  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ \
          --verbose \
          --cov=kuzu_memory \
          --cov-report=xml \
          --junit-xml=test-results-integration.xml \
          --tb=short \
          --timeout=300
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: test-results-integration.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration
        name: integration-tests

  # End-to-End Tests - Full system validation
  e2e-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run E2E tests
      run: |
        python -m pytest tests/e2e/ \
          --verbose \
          --junit-xml=test-results-e2e.xml \
          --tb=short \
          --timeout=600
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: test-results-e2e.xml

  # Performance Benchmarks - Validate performance requirements
  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run performance benchmarks
      run: |
        python -m pytest tests/benchmarks/ \
          --verbose \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --junit-xml=test-results-benchmarks.xml \
          --tb=short
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
          test-results-benchmarks.xml
    
    - name: Performance regression check
      run: |
        python scripts/check_performance_regression.py benchmark-results.json

  # Regression Tests - Run on schedule and releases
  regression-tests:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'push' && contains(github.ref, 'refs/tags/')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run regression tests
      run: |
        python -m pytest tests/regression/ \
          --verbose \
          --junit-xml=test-results-regression.xml \
          --tb=short \
          --timeout=1200
    
    - name: Upload regression test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: regression-test-results
        path: test-results-regression.xml
    
    - name: Create regression report
      if: always()
      run: |
        python scripts/generate_regression_report.py \
          --test-results test-results-regression.xml \
          --output regression-report.md
    
    - name: Upload regression report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: regression-report
        path: regression-report.md

  # Code Quality Checks
  code-quality:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run black (code formatting)
      run: |
        black --check --diff src/ tests/
    
    - name: Run isort (import sorting)
      run: |
        isort --check-only --diff src/ tests/
    
    - name: Run flake8 (linting)
      run: |
        flake8 src/ tests/ --max-line-length=100 --extend-ignore=E203,W503
    
    - name: Run mypy (type checking)
      run: |
        mypy src/kuzu_memory --ignore-missing-imports
    
    - name: Run bandit (security)
      run: |
        bandit -r src/ -f json -o bandit-report.json
    
    - name: Upload security report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-report
        path: bandit-report.json

  # Test Summary and Reporting
  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests, code-quality]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install junitparser pytest-html
    
    - name: Generate test summary
      run: |
        python scripts/generate_test_summary.py \
          --unit-results unit-test-results-*/test-results-unit.xml \
          --integration-results integration-test-results/test-results-integration.xml \
          --e2e-results e2e-test-results/test-results-e2e.xml \
          --benchmark-results benchmark-results/benchmark-results.json \
          --output test-summary.html
    
    - name: Upload test summary
      uses: actions/upload-artifact@v4
      with:
        name: test-summary
        path: test-summary.html
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read test summary (simplified version for PR comment)
          let summary = '## üß™ Test Results Summary\n\n';
          
          // Add basic test status
          const testJobs = [
            { name: 'Unit Tests', needed: true },
            { name: 'Integration Tests', needed: true },
            { name: 'E2E Tests', needed: true },
            { name: 'Performance Tests', needed: true },
            { name: 'Code Quality', needed: true }
          ];
          
          for (const job of testJobs) {
            const status = job.needed ? '‚úÖ' : '‚è≠Ô∏è';
            summary += `- ${status} ${job.name}\n`;
          }
          
          summary += '\nüìä Detailed results available in the [Actions tab](' + 
                    context.payload.pull_request.html_url.replace('/pull/', '/actions') + ')';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

  # Notify on failure
  notify-failure:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    if: failure() && (github.event_name == 'push' && github.ref == 'refs/heads/main')
    
    steps:
    - name: Notify team of test failures
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `üö® Test Failures on Main Branch - ${new Date().toISOString().split('T')[0]}`,
            body: `
            ## Test Failure Alert
            
            **Branch:** ${context.ref}
            **Commit:** ${context.sha}
            **Workflow:** ${context.workflow}
            **Run:** ${context.runNumber}
            
            One or more test suites have failed on the main branch. Please investigate immediately.
            
            **Failed Jobs:**
            - Check the [Actions tab](${context.payload.repository.html_url}/actions) for details
            
            **Next Steps:**
            1. Review the failing tests
            2. Fix the issues or revert the problematic commit
            3. Ensure all tests pass before merging future PRs
            
            cc: @team-leads
            `,
            labels: ['bug', 'urgent', 'ci-failure']
          });
